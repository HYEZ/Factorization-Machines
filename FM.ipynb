{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1d113b672646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m                               valid_data='../dataset/test.txt')\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1d113b672646>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mTrain\u001b[0m \u001b[0mFM\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mby\u001b[0m \u001b[0mGradient\u001b[0m \u001b[0mDescent\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mL2\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mtrain_y_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1d113b672646>\u001b[0m in \u001b[0;36m_load_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mvalid_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/train.txt'"
     ]
    }
   ],
   "source": [
    "# Factorization Machine Classifier : It is only for study\n",
    "# Copyright : yoonkt200@gmail.com\n",
    "# Apache License 2.0\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class FactorizationMachine():\n",
    "    \"\"\"\n",
    "    This Class is implementation of this paper : https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "    Just a example of FM Algorithm, not for production.\n",
    "    -----\n",
    "    Only simple methods are available.\n",
    "    e.g 1 : batch training, adagrad optimizer, parallel training are not supported.\n",
    "    e.g 2 : simple optimizer Stochastic Gradient Descent with L2 Regularization.\n",
    "    e.g 3 : using titanic dataset on local memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, k, lr, l2_reg, l2_lambda, epoch, early_stop_window, train_data, valid_data):\n",
    "        \"\"\"\n",
    "        :param k: number of latent vector\n",
    "        :param lr: learning rate\n",
    "        :param l2_reg: bool parameter for L2 regularization\n",
    "        :param l2_lambda: lambda of L2 regularization\n",
    "        :param epoch: training epoch\n",
    "        :param train_data: path of train data\n",
    "        :param valid_data: path of valid data\n",
    "        \"\"\"\n",
    "        self._k = k\n",
    "        self._lr = lr\n",
    "        self._l2_reg = l2_reg\n",
    "        self._l2_lambda = l2_lambda\n",
    "        self._epoch = epoch\n",
    "        self._early_stop_window = early_stop_window\n",
    "        self._train_file_path = train_data\n",
    "        self._valid_file_path = valid_data\n",
    "        self._valid_loss_list = []\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        \"\"\"\n",
    "        1. load dataset to memory from train/valid path\n",
    "        2. find max index in dataset for w's vector size\n",
    "        \"\"\"\n",
    "        # load data\n",
    "        train_file = open(self._train_file_path, 'r')\n",
    "        valid_file = open(self._valid_file_path, 'r')\n",
    "        self._train_data = train_file.read().split('\\n')\n",
    "        self._valid_data = valid_file.read().split('\\n')\n",
    "        train_file.close()\n",
    "        valid_file.close()\n",
    "\n",
    "        # find max index\n",
    "        self.feature_max_index = 0\n",
    "        print(\"Start to init FM vectors.\")\n",
    "        for row in self._train_data:\n",
    "            for element in row.split(\" \")[1:]:\n",
    "                index = int(element.split(\":\")[0])\n",
    "                if self.feature_max_index < index:\n",
    "                    self.feature_max_index = index\n",
    "\n",
    "        for row in self._valid_data:\n",
    "            for element in row.split(\" \")[1:]:\n",
    "                index = int(element.split(\":\")[0])\n",
    "                if self.feature_max_index < index:\n",
    "                    self.feature_max_index = index\n",
    "\n",
    "        # init FM vectors\n",
    "        self._init_vectors()\n",
    "        print(\"Finish init FM vectors.\")\n",
    "\n",
    "    def _init_vectors(self):\n",
    "        \"\"\"\n",
    "        1. initialize FM vectors\n",
    "        2. Conduct naive transformation libsvm format txt data to numpy training sample.\n",
    "        \"\"\"\n",
    "        self.w = np.random.randn(self.feature_max_index+1)\n",
    "        self.v = np.random.randn(self.feature_max_index+1, self._k)\n",
    "        self.train_x_data = []\n",
    "        self.train_y_data = np.zeros((len(self._train_data)))\n",
    "        self.valid_x_data = []\n",
    "        self.valid_y_data = np.zeros((len(self._valid_data)))\n",
    "\n",
    "        # make numpy dataset\n",
    "        for n, row in enumerate(self._train_data):\n",
    "            element = row.split(\" \")\n",
    "            self.train_y_data[n] = int(element[0])\n",
    "            self.train_x_data.append([np.array([int(pair.split(\":\")[0]) for pair in element[1:]]),\n",
    "                                      np.array([int(pair.split(\":\")[1]) for pair in element[1:]])])\n",
    "\n",
    "        for n, row in enumerate(self._valid_data):\n",
    "            element = row.split(\" \")\n",
    "            self.valid_y_data[n] = int(element[0])\n",
    "            self.valid_x_data.append([np.array([int(pair.split(\":\")[0]) for pair in element[1:]]),\n",
    "                                      np.array([int(pair.split(\":\")[1]) for pair in element[1:]])])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train FM model by Gradient Descent with L2 regularization\n",
    "        \"\"\"\n",
    "        self._load_dataset()\n",
    "        for epoch_num in range(1, self._epoch):\n",
    "            train_y_hat = self.predict(data=self.train_x_data)\n",
    "            valid_y_hat = self.predict(data=self.valid_x_data)\n",
    "            train_loss = self._get_loss(y_data=self.train_y_data, y_hat=train_y_hat)\n",
    "            valid_loss = self._get_loss(y_data=self.valid_y_data, y_hat=valid_y_hat)\n",
    "            train_auc = roc_auc_score(self.train_y_data, train_y_hat)\n",
    "            valid_auc = roc_auc_score(self.valid_y_data, valid_y_hat)\n",
    "            self._print_learning_info(epoch=epoch_num, train_loss=train_loss, valid_loss=valid_loss,\n",
    "                                      train_auc=train_auc, valid_auc=valid_auc)\n",
    "            if self._check_early_stop(valid_loss=valid_loss):\n",
    "                print(\"Early stop at epoch:\", epoch_num)\n",
    "                return 0\n",
    "\n",
    "            self._stochastic_gradient_descent(self.train_x_data, self.train_y_data)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Implementation of FM model's equation on O(kmd)\n",
    "        -----\n",
    "        Numpy array shape : (n, [index of md], [value of md])\n",
    "        md : none-zero feature\n",
    "        \"\"\"\n",
    "        num_data = len(data)\n",
    "        scores = np.zeros(num_data)\n",
    "        for n in range(num_data):\n",
    "            feat_idx = data[n][0]\n",
    "            val = data[n][1]\n",
    "\n",
    "            # linear feature score\n",
    "            linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
    "\n",
    "            # factorized feature score\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "            cross_sum = np.sum(vx, axis=0)\n",
    "            square_sum = np.sum(vx * vx, axis=0)\n",
    "            cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
    "\n",
    "            # Model's equation\n",
    "            scores[n] = linear_feature_score + cross_feature_score\n",
    "\n",
    "        # Sigmoid transformation for binary classification\n",
    "        scores = 1.0 / (1.0 + np.exp(-scores))\n",
    "        return scores\n",
    "\n",
    "    def _get_loss(self, y_data, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate loss with L2 regularization (two type of coeficient - w,v)\n",
    "        \"\"\"\n",
    "        l2_norm = 0\n",
    "        if self._l2_reg:\n",
    "            w_norm = np.sqrt(np.sum(np.square(self.w)))\n",
    "            v_norm = np.sqrt(np.sum(np.square(self.v)))\n",
    "            l2_norm = self._l2_lambda * (w_norm + v_norm)\n",
    "        return -1 * np.sum( (y_data * np.log(y_hat)) + ((1 - y_data) * np.log(1 - y_hat)) ) + l2_norm\n",
    "\n",
    "    def _check_early_stop(self, valid_loss):\n",
    "        self._valid_loss_list.append(valid_loss)\n",
    "        if len(self._valid_loss_list) > 5:\n",
    "            prev_loss = self._valid_loss_list[len(self._valid_loss_list) - self._early_stop_window]\n",
    "            curr_loss = valid_loss\n",
    "            if prev_loss < curr_loss:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _print_learning_info(self, epoch, train_loss, valid_loss, train_auc, valid_auc):\n",
    "        print(\"epoch:\", epoch, \"||\", \"train_loss:\", train_loss, \"||\", \"valid_loss:\", valid_loss,\n",
    "              \"||\", \"Train AUC:\", train_auc, \"||\", \"Test AUC:\", valid_auc)\n",
    "\n",
    "\n",
    "    def _stochastic_gradient_descent(self, x_data, y_data):\n",
    "        \"\"\"\n",
    "        Update each coefs (w, v) by Gradient Descent\n",
    "        \"\"\"\n",
    "        for data, y in zip(x_data, y_data):\n",
    "            feat_idx = data[0]\n",
    "            val = data[1]\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "\n",
    "            # linear feature score\n",
    "            linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
    "\n",
    "            # factorized feature score\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "            cross_sum = np.sum(vx, axis=0)\n",
    "            square_sum = np.sum(vx * vx, axis=0)\n",
    "            cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
    "\n",
    "            # Model's equation\n",
    "            score = linear_feature_score + cross_feature_score\n",
    "            y_hat = 1.0 / (1.0 + np.exp(-score))\n",
    "            cost = y_hat - y\n",
    "\n",
    "            if self._l2_reg:\n",
    "                self.w[feat_idx] = self.w[feat_idx] - cost * self._lr * (val + self._l2_lambda * self.w[feat_idx])\n",
    "                self.v[feat_idx] = self.v[feat_idx] - cost * self._lr * ((sum(vx) * (val.reshape(-1, 1)) - (vx * (val.reshape(-1, 1)))) + self._l2_lambda * self.v[feat_idx])\n",
    "            else:\n",
    "                self.w[feat_idx] = self.w[feat_idx] - cost * self._lr * val\n",
    "                self.v[feat_idx] = self.v[feat_idx] - cost * self._lr * (sum(vx) * (val.reshape(-1, 1)) - (vx * (val.reshape(-1, 1))))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fm = FactorizationMachine(k=4,\n",
    "                              lr=0.001,\n",
    "                              l2_reg=True,\n",
    "                              l2_lambda=0.0001,\n",
    "                              epoch=200,\n",
    "                              early_stop_window=3,\n",
    "                              train_data='../dataset/train.txt',\n",
    "                              valid_data='../dataset/test.txt')\n",
    "\n",
    "    fm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
